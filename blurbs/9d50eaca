Playing Atari with Deep Reinforcement Learning

The goal of the project was to create an agent which could successfully learn to play as many games as possible, given high-dimensional sensory input. The method was applied on seven ATARI games - Beam Rider, Breakout, Enduro, Pong, Q*bert, Seaquest, Space Invaders.

The application was created using deep reinforcement learning and the network was trained with a variant of the Q-learning algorithm, with stochastic gradient descent to update the weights. The agent interacts with its environment in discrete time steps. It sees and selects actions on every kth frame and its last action is repeated on skipped frames. This allows the agent to play more games without significantly increasing the runtime of the application.  At each time step where an action is chosen, the agent receives an observation. It then chooses an action, from the set of possible actions. The environment moves to a new state. The rewards given can depend on a set of previous actions and may only be received after thousands of time steps have elapsed. The goal of a reinforcement learning agent is to collect as much reward as possible.

The same network architecture, learning algorithm and parameter settings were used across the 7 games, without including game specific information. The agent did not have access to the internal state of the emulator and was only given the same information a human player would be given - raw video input, rewards for its actions and the set of possible actions. This approach was successful for six of the seven games it was tested on and in three of the games it performed better then a human expert.
